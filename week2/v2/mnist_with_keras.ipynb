{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "mnist_with_keras.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qze9t_8nx3a6",
        "outputId": "cba04c4f-18d2-46e7-e8f3-b8fd6f68e84a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# set tf 1.x for colab\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlyiBLMOx_ad",
        "outputId": "e044631c-8a41-4a04-c0d6-bc7924a918b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/Nik0la-i/intro-to-dl/master/setup_google_colab.py\n",
        "import setup_google_colab\n",
        "setup_google_colab.setup_week2()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-26 14:05:55--  https://raw.githubusercontent.com/Nik0la-i/intro-to-dl/master/setup_google_colab.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3636 (3.6K) [text/plain]\n",
            "Saving to: ‘setup_google_colab.py’\n",
            "\n",
            "\rsetup_google_colab.   0%[                    ]       0  --.-KB/s               \rsetup_google_colab. 100%[===================>]   3.55K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-08-26 14:05:55 (50.8 MB/s) - ‘setup_google_colab.py’ saved [3636/3636]\n",
            "\n",
            "**************************************************\n",
            "inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "**************************************************\n",
            "cifar-10-batches-py.tar.gz\n",
            "**************************************************\n",
            "mnist.npz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKwRVplyyHAA",
        "outputId": "3dd0f3df-36af-4f27-d87d-17b26a57c3f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 945
        }
      },
      "source": [
        "!pip3 uninstall keras\n",
        "!pip3 install keras==2.1.6\n",
        "!pip3 install h5py==2.10.0"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found existing installation: keras 2.6.0\n",
            "Uninstalling keras-2.6.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/keras-2.6.0.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/*\n",
            "  Would not remove (might be manually added):\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/applications/resnet50.py\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/backend/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/backend/cntk_backend.py\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/backend/common.py\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/backend/theano_backend.py\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/engine/topology.py\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/initializers.py\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/layers/normalization.py\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/legacy/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/legacy/interfaces.py\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/legacy/layers.py\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/legacy/models.py\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/objectives.py\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/utils/test_utils.py\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled keras-2.6.0\n",
            "Collecting keras==2.1.6\n",
            "  Downloading Keras-2.1.6-py2.py3-none-any.whl (339 kB)\n",
            "\u001b[K     |████████████████████████████████| 339 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.1.6) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.1.6) (3.1.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.6) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.6) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.6) (1.15.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.1.6) (1.5.2)\n",
            "Installing collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: Keras 2.0.6\n",
            "    Uninstalling Keras-2.0.6:\n",
            "      Successfully uninstalled Keras-2.0.6\n",
            "Successfully installed keras-2.1.6\n",
            "Collecting h5py==2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.15.0)\n",
            "Installing collected packages: h5py\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "Successfully installed h5py-2.10.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "h5py"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUUdzHbyx3a-"
      },
      "source": [
        "# MNIST digits classification with Keras\n",
        "\n",
        "We don't expect you to code anything here because you've already solved it with TensorFlow.\n",
        "\n",
        "But you can appreciate how simpler it is with Keras.\n",
        "\n",
        "We'll be happy if you play around with the architecture though, there're some tips at the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BVTkJcHx3a_"
      },
      "source": [
        "<img src=\"https://github.com/Nik0la-i/intro-to-dl/blob/master/week2/v2/images/mnist_sample.png?raw=1\" style=\"width:30%\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Du7wUvU0x3bA",
        "outputId": "09271b07-8dae-47ce-c54b-7267a4f58532",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "print(\"We're using TF\", tf.__version__)\n",
        "import keras\n",
        "print(\"We are using Keras\", keras.__version__)\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"../..\")\n",
        "import keras_utils\n",
        "from keras_utils import reset_tf_session"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We're using TF 1.15.2\n",
            "We are using Keras 2.1.6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idPS-7NYx3bB"
      },
      "source": [
        "# Look at the data\n",
        "\n",
        "In this task we have 50000 28x28 images of digits from 0 to 9.\n",
        "We will train a classifier on this data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tya2nmxcx3bC",
        "outputId": "caa88d0a-2396-42ba-c520-58b0d0dd0a34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import preprocessed_mnist\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = preprocessed_mnist.load_dataset()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeOypqvMx3bD",
        "outputId": "924594d5-d7c1-4484-9a01-4023cd1a5831",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        }
      },
      "source": [
        "# X contains rgb values divided by 255\n",
        "print(\"X_train [shape %s] sample patch:\\n\" % (str(X_train.shape)), X_train[1, 15:20, 5:10])\n",
        "print(\"A closeup of a sample patch:\")\n",
        "plt.imshow(X_train[1, 15:20, 5:10], cmap=\"Greys\")\n",
        "plt.show()\n",
        "print(\"And the whole sample:\")\n",
        "plt.imshow(X_train[1], cmap=\"Greys\")\n",
        "plt.show()\n",
        "print(\"y_train [shape %s] 10 samples:\\n\" % (str(y_train.shape)), y_train[:10])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train [shape (50000, 28, 28)] sample patch:\n",
            " [[0.         0.29803922 0.96470588 0.98823529 0.43921569]\n",
            " [0.         0.33333333 0.98823529 0.90196078 0.09803922]\n",
            " [0.         0.33333333 0.98823529 0.8745098  0.        ]\n",
            " [0.         0.33333333 0.98823529 0.56862745 0.        ]\n",
            " [0.         0.3372549  0.99215686 0.88235294 0.        ]]\n",
            "A closeup of a sample patch:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "And the whole sample:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "y_train [shape (50000,)] 10 samples:\n",
            " [5 0 4 1 9 2 1 3 1 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVSojwlKx3bE",
        "outputId": "32428d5b-1f01-4cc2-a4ac-dd227d9428ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# flatten images\n",
        "X_train_flat = X_train.reshape((X_train.shape[0], -1))\n",
        "print(X_train_flat.shape)\n",
        "\n",
        "X_val_flat = X_val.reshape((X_val.shape[0], -1))\n",
        "print(X_val_flat.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 784)\n",
            "(10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b21WSuzsx3bG",
        "outputId": "51c0e512-b5e5-4bf9-ce77-5351ad7c2ad3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# one-hot encode the target\n",
        "y_train_oh = keras.utils.to_categorical(y_train, 10)\n",
        "y_val_oh = keras.utils.to_categorical(y_val, 10)\n",
        "\n",
        "print(y_train_oh.shape)\n",
        "print(y_train_oh[:3], y_train[:3])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 10)\n",
            "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] [5 0 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhMEyCaxx3bH",
        "outputId": "3618f5ae-a68d-49b3-da43-4a16677fcc94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# building a model with keras\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.models import Sequential\n",
        "\n",
        "# we still need to clear a graph though\n",
        "s = reset_tf_session()\n",
        "\n",
        "model = Sequential()  # it is a feed-forward network without loops like in RNN\n",
        "model.add(Dense(256, input_shape=(784,)))  # the first layer must specify the input shape (replacing placeholders)\n",
        "model.add(Activation('sigmoid'))\n",
        "model.add(Dense(256))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/keras_utils.py:68: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:89: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:92: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:96: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/keras_utils.py:75: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/keras_utils.py:77: The name tf.InteractiveSession is deprecated. Please use tf.compat.v1.InteractiveSession instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNxmbYTDx3bI",
        "outputId": "26fdf897-e244-491f-eb3b-f37a87c1ab93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# you can look at all layers and parameter count\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 256)               200960    \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 269,322\n",
            "Trainable params: 269,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TA9A8Lbx3bI",
        "outputId": "0e127319-4380-4f4d-8623-8f8e63eb165e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# now we \"compile\" the model specifying the loss and optimizer\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy', # this is our cross-entropy\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']  # report accuracy during training\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3014: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyHBGHlX5uHF"
      },
      "source": [
        "class CustomCallback(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"Starting training; got log keys: {}\".format(keys))\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"Stop training; got log keys: {}\".format(keys))\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"Start epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"End epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
        "\n",
        "    def on_test_begin(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"Start testing; got log keys: {}\".format(keys))\n",
        "\n",
        "    def on_test_end(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"Stop testing; got log keys: {}\".format(keys))\n",
        "\n",
        "    def on_predict_begin(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"Start predicting; got log keys: {}\".format(keys))\n",
        "\n",
        "    def on_predict_end(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"Stop predicting; got log keys: {}\".format(keys))\n",
        "\n",
        "    def on_train_batch_begin(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"...Training: start of batch {}; got log keys: {}\".format(batch, keys))\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"...Training: end of batch {}; got log keys: {}\".format(batch, keys))\n",
        "\n",
        "    def on_test_batch_begin(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"...Evaluating: start of batch {}; got log keys: {}\".format(batch, keys))\n",
        "\n",
        "    def on_test_batch_end(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"...Evaluating: end of batch {}; got log keys: {}\".format(batch, keys))\n",
        "\n",
        "    def on_predict_batch_begin(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"...Predicting: start of batch {}; got log keys: {}\".format(batch, keys))\n",
        "\n",
        "    def on_predict_batch_end(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"...Predicting: end of batch {}; got log keys: {}\".format(batch, keys))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8rkdZ3Xx3bI",
        "outputId": "6f8aa500-c0c3-4dfa-b4bd-17c030127aae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# and now we can fit the model with model.fit()\n",
        "# and we don't have to write loops and batching manually as in TensorFlow\n",
        "model.fit(\n",
        "    X_train_flat, \n",
        "    y_train_oh,\n",
        "    batch_size=512, \n",
        "    epochs=40,\n",
        "    validation_data=(X_val_flat, y_val_oh),\n",
        "    callbacks=[CustomCallback()],\n",
        "    verbose=0\n",
        ")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting training; got log keys: []\n",
            "Start epoch 0 of training; got log keys: []\n",
            "End epoch 0 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 1 of training; got log keys: []\n",
            "End epoch 1 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 2 of training; got log keys: []\n",
            "End epoch 2 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 3 of training; got log keys: []\n",
            "End epoch 3 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 4 of training; got log keys: []\n",
            "End epoch 4 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 5 of training; got log keys: []\n",
            "End epoch 5 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 6 of training; got log keys: []\n",
            "End epoch 6 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 7 of training; got log keys: []\n",
            "End epoch 7 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 8 of training; got log keys: []\n",
            "End epoch 8 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 9 of training; got log keys: []\n",
            "End epoch 9 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 10 of training; got log keys: []\n",
            "End epoch 10 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 11 of training; got log keys: []\n",
            "End epoch 11 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 12 of training; got log keys: []\n",
            "End epoch 12 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 13 of training; got log keys: []\n",
            "End epoch 13 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 14 of training; got log keys: []\n",
            "End epoch 14 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 15 of training; got log keys: []\n",
            "End epoch 15 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 16 of training; got log keys: []\n",
            "End epoch 16 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 17 of training; got log keys: []\n",
            "End epoch 17 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 18 of training; got log keys: []\n",
            "End epoch 18 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 19 of training; got log keys: []\n",
            "End epoch 19 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 20 of training; got log keys: []\n",
            "End epoch 20 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 21 of training; got log keys: []\n",
            "End epoch 21 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 22 of training; got log keys: []\n",
            "End epoch 22 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 23 of training; got log keys: []\n",
            "End epoch 23 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 24 of training; got log keys: []\n",
            "End epoch 24 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 25 of training; got log keys: []\n",
            "End epoch 25 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 26 of training; got log keys: []\n",
            "End epoch 26 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 27 of training; got log keys: []\n",
            "End epoch 27 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 28 of training; got log keys: []\n",
            "End epoch 28 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 29 of training; got log keys: []\n",
            "End epoch 29 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 30 of training; got log keys: []\n",
            "End epoch 30 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 31 of training; got log keys: []\n",
            "End epoch 31 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 32 of training; got log keys: []\n",
            "End epoch 32 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 33 of training; got log keys: []\n",
            "End epoch 33 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 34 of training; got log keys: []\n",
            "End epoch 34 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 35 of training; got log keys: []\n",
            "End epoch 35 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 36 of training; got log keys: []\n",
            "End epoch 36 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 37 of training; got log keys: []\n",
            "End epoch 37 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 38 of training; got log keys: []\n",
            "End epoch 38 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Start epoch 39 of training; got log keys: []\n",
            "End epoch 39 of training; got log keys: ['val_loss', 'val_acc', 'loss', 'acc']\n",
            "Stop training; got log keys: []\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9401a18490>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnW5g4Lfx3bJ"
      },
      "source": [
        "# Here're the notes for those who want to play around here\n",
        "\n",
        "Here are some tips on what you could do:\n",
        "\n",
        " * __Network size__\n",
        "   * More neurons, \n",
        "   * More layers, ([docs](https://keras.io/))\n",
        "\n",
        "   * Other nonlinearities in the hidden layers\n",
        "     * tanh, relu, leaky relu, etc\n",
        "   * Larger networks may take more epochs to train, so don't discard your net just because it could didn't beat the baseline in 5 epochs.\n",
        "\n",
        "\n",
        " * __Early Stopping__\n",
        "   * Training for 100 epochs regardless of anything is probably a bad idea.\n",
        "   * Some networks converge over 5 epochs, others - over 500.\n",
        "   * Way to go: stop when validation score is 10 iterations past maximum\n",
        "     \n",
        "\n",
        " * __Faster optimization__\n",
        "   * rmsprop, nesterov_momentum, adam, adagrad and so on.\n",
        "     * Converge faster and sometimes reach better optima\n",
        "     * It might make sense to tweak learning rate/momentum, other learning parameters, batch size and number of epochs\n",
        "\n",
        "\n",
        " * __Regularize__ to prevent overfitting\n",
        "   * Add some L2 weight norm to the loss function, theano will do the rest\n",
        "     * Can be done manually or via - https://keras.io/regularizers/\n",
        "   \n",
        "   \n",
        " * __Data augmemntation__ - getting 5x as large dataset for free is a great deal\n",
        "   * https://keras.io/preprocessing/image/\n",
        "   * Zoom-in+slice = move\n",
        "   * Rotate+zoom(to remove black stripes)\n",
        "   * any other perturbations\n",
        "   * Simple way to do that (if you have PIL/Image): \n",
        "     * ```from scipy.misc import imrotate,imresize```\n",
        "     * and a few slicing\n",
        "   * Stay realistic. There's usually no point in flipping dogs upside down as that is not the way you usually see them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxo-ZSyux3bJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}